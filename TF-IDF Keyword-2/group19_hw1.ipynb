{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent, n_gram, stopword):\n",
    "    sent = re.sub(r'[^\\w]','',sent)\n",
    "    sent = re.sub(r'[A-Za-z0-9]','',sent)\n",
    "    return_list = []\n",
    "    for i in range(len(sent) - n_gram + 1):\n",
    "        w = sent[i:i+n_gram]\n",
    "        if w not in stopword:\n",
    "            return_list.append(w)\n",
    "    _return = pd.DataFrame(return_list, columns = ['word']) \n",
    "    return _return\n",
    "#retrun as df\n",
    "# word \n",
    "# -----\n",
    "# A\n",
    "# B\n",
    "# A \n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create case_list and article_list\n",
    "class case:\n",
    "    def __init__(self, topic):\n",
    "        self.topic = topic\n",
    "        self._articles = [] # the index of aricles belong the topic [7,9...]\n",
    "        self._words = {} # {2:df, 3:df ...} for case \n",
    "        \n",
    "        \n",
    "    def add_articles(self, _index):\n",
    "        self._articles.append(_index)\n",
    "        \n",
    "class article:\n",
    "    def __init__(self, _index):\n",
    "        self.index = _index \n",
    "        self._ngram = {} # {2:df, 3:df ...} for single article\n",
    "\n",
    "    def addNgram(self, n, gram_list): # will be called with global funcion preprocess\n",
    "        self._ngram[n] = gram_list \n",
    "        self._ngram[n] = self._ngram[n].groupby('word').size().reset_index(name = 'tf')\n",
    "        \n",
    "        # funcion size()\n",
    "        # return the size within each group such like\n",
    "        # word    tf\n",
    "        # ---------- \n",
    "        # A    2\n",
    "        # B    1 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data with stopword \"stopwords.txt\" \n",
    "# run about 10s\n",
    "collections =pd.read_excel('bda2020_hw1/hw1_text.xlsx', index_col = 0)\n",
    "stopword = []\n",
    "n = len(collections)\n",
    "with open ('bda2020_hw1/stopwords.txt','r',encoding = 'utf-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopword.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1min7s \n",
    "# Identify articles that belong to different topics\n",
    "\n",
    "topics = ['銀行','信用卡','匯率','台積電','台灣','日本']\n",
    "\n",
    "case_list = [case(i) for i in topics]    \n",
    "article_list = [article(index) for index in range(n+1)] # extra new for index = 0 \n",
    "\n",
    "deal = [] # remember the articles index will be dealed with\n",
    "\n",
    "for c in case_list: # for 6\n",
    "    for index, val in collections.iterrows(): # for 90000\n",
    "        if c.topic in val[\"標題\"] + val[\"內容\"]:\n",
    "            c.add_articles(index)\n",
    "            deal.append(index)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "銀行\n",
      "6674\n",
      "信用卡\n",
      "653\n",
      "匯率\n",
      "1951\n",
      "台積電\n",
      "1738\n",
      "台灣\n",
      "27232\n",
      "日本\n",
      "8235\n"
     ]
    }
   ],
   "source": [
    "# take a look at the number of articles under each topic\n",
    "\n",
    "N = len(collections)\n",
    "n = {}\n",
    "for i in case_list:\n",
    "    n[i.topic] = len(i._articles)\n",
    "    print(i.topic)\n",
    "    print(n[i.topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 15min  \n",
    "# cut n-gram for the articles will be dealed with\n",
    "K = [2, 3, 4, 5, 6]\n",
    "for index, val in collections.iterrows(): # for 90000\n",
    "    if index in deal:  # for about 45000\n",
    "        for k in K: \n",
    "            article_list[index].addNgram(k,preprocess(val[\"標題\"]+val[\"內容\"],k,stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute df + tf\n",
    "\n",
    "# use the two loop to control the range of data to avoid to caculate too much data at one time\n",
    "# it will run about 2 hours without discard QQ\n",
    "# so we run the different topic in differnt group memeber's computer\n",
    "\n",
    "# take \" 台灣 \" case_list[4] for example\n",
    "\n",
    "\n",
    "for c in case_list[1:2]:\n",
    "    for k in K[:1]:\n",
    "        cut = 0    # just for checking where it is caculating now\n",
    "        \n",
    "        c._words[k] = article_list[c._articles[0]]._ngram[k].copy()\n",
    "        c._words[k].insert(1,'case_df',0)\n",
    "        c._words[k].insert(2,'case_tf',0)\n",
    "        \n",
    "        for i in c._articles: # for 6700 articles\n",
    "            if i == c._articles[0]: #since we have already copy  \n",
    "                continue\n",
    "                \n",
    "            c._words[k] = c._words[k].merge(article_list[i]._ngram[k], how=\"outer\", on =\"word\")\n",
    "            ###################################\n",
    "            #  word case_df  case_tf tf_x tf_y\n",
    "            #   A     0       0        2   NaN\n",
    "            #   B     NaN    NaN      NaN   3\n",
    "            #   C     0       0        5    1 \n",
    "            #   D     NaN    NaN      NaN   2\n",
    "            ###################################\n",
    "                  \n",
    "            cut += 1\n",
    "            if cut % 100 == 0:\n",
    "                print('cut',cut)\n",
    "                \n",
    "            if cut % 2 == 0 or i == c._articles[-1]: # merge one articles into the c_word[k]\n",
    "                #df  (with function fillna and count)\n",
    "                c._words[k]['case_df'].fillna(value = 0, inplace =True)\n",
    "                c._words[k]['case_tf'].fillna(value = 0, inplace =True)\n",
    "                 \n",
    "                c._words[k]['case_df']+= c._words[k].count(axis = \"columns\").values #　Count non-NA cells per row\n",
    "                c._words[k]['case_df']-= 3 # sice it will count \"word\" \"case_df\" \"case_tf\"　as non-NA cells　\n",
    "\n",
    "                #tf\n",
    "                c._words[k]['case_tf'] = c._words[k].sum(axis = 1) #　Count total numbers per row\n",
    "                c._words[k]['case_tf']-= c._words[k]['case_df']   # sice it will  extractly add case_df　\n",
    "                c._words[k].drop(c._words[k].columns[3:],axis =1,inplace =True) # throw away tf_x tf_y\n",
    "                \n",
    "                # c._words[k] = c._words[k][c._words[k]['case_df']>discard] \n",
    "                # Ignore the words that df is too small to accelerate\n",
    "                \n",
    "                ###################################\n",
    "                #  word case_df  case_tf\n",
    "                #   A     1       2       \n",
    "                #   B     1       3     \n",
    "                #   C     2       6       \n",
    "                #   D     1       2       \n",
    "                ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台灣 k = 2\n",
      "      word  case_df  case_tf    tf-idf\n",
      "0       一年   1450.0   1911.0  5.453091\n",
      "1       一片    233.0    257.0  7.050799\n",
      "2       不少   1496.0   1794.0  5.360446\n",
      "3       世界   3522.0   5863.0  4.235474\n",
      "4       主題   1136.0   1647.0  5.817777\n",
      "...    ...      ...      ...       ...\n",
      "74225   長巫      1.0      1.0  4.435080\n",
      "74226   陸維      1.0      1.0  4.435080\n",
      "74227   項任      1.0      1.0  4.435080\n",
      "74228   願與      1.0      1.0  4.435080\n",
      "74229   麼期      1.0      1.0  4.435080\n",
      "\n",
      "[74230 rows x 4 columns]\n",
      "台灣 k = 3\n",
      "      word  case_df  case_tf    tf-idf\n",
      "0      台灣最    630.0    731.0  6.320360\n",
      "1      新的一     54.0     63.0  7.565738\n",
      "2      的一年    114.0    143.0  7.503940\n",
      "3      的開始    120.0    135.0  7.374748\n",
      "4      下半年    838.0   1368.0  6.253082\n",
      "...    ...      ...      ...       ...\n",
      "89206  非常趨      1.0      1.0  4.435080\n",
      "89207  項任命      1.0      1.0  4.435080\n",
      "89208  願與大      1.0      1.0  4.435080\n",
      "89209  顯示她      1.0      1.0  4.435080\n",
      "89210  麼期待      1.0      1.0  4.435080\n",
      "\n",
      "[89211 rows x 4 columns]\n",
      "台灣 k = 4\n",
      "       word  case_df  case_tf    tf-idf\n",
      "0      新的一年     37.0     45.0  7.606436\n",
      "1      年下半年    285.0    323.0  6.949045\n",
      "2      中小企業    392.0    787.0  7.175581\n",
      "3      國旗圍巾      6.0      9.0  7.146525\n",
      "4      表示台灣   1558.0   1712.0  5.260180\n",
      "...     ...      ...      ...       ...\n",
      "65202  非常趨冷      1.0      1.0  4.435080\n",
      "65203  項任命一      1.0      1.0  4.435080\n",
      "65204  願與大陸      1.0      1.0  4.435080\n",
      "65205  顯示她並      1.0      1.0  4.435080\n",
      "65206  黨當局任      1.0      1.0  4.435080\n",
      "\n",
      "[65207 rows x 4 columns]\n",
      "台灣 k = 5\n",
      "        word  case_df  case_tf    tf-idf\n",
      "0      首席執行官     11.0     13.0  7.174062\n",
      "1      任內最後一     26.0     28.0  7.390677\n",
      "2      元旦升旗典     12.0     14.0  7.202187\n",
      "3      內最後一次     21.0     23.0  7.351729\n",
      "4      到國際社會     13.0     15.0  7.227095\n",
      "...      ...      ...      ...       ...\n",
      "45513  非常趨冷的      1.0      1.0  4.435080\n",
      "45514  項任命一直      1.0      1.0  4.435080\n",
      "45515  願與大陸維      1.0      1.0  4.435080\n",
      "45516  顯示她並沒      1.0      1.0  4.435080\n",
      "45517  黨當局任命      1.0      1.0  4.435080\n",
      "\n",
      "[45518 rows x 4 columns]\n",
      "台灣 k = 6\n",
      "         word  case_df  case_tf    tf-idf\n",
      "0      任內最後一次     21.0     23.0  7.351729\n",
      "1      元旦升旗典禮     12.0     14.0  7.202187\n",
      "2      內最後一次元      6.0      7.0  6.747391\n",
      "3      到國際社會肯      3.0      4.0  6.340887\n",
      "4      受到國際社會      5.0      7.0  6.893488\n",
      "...       ...      ...      ...       ...\n",
      "34781  非常趨冷的情      1.0      1.0  4.435080\n",
      "34782  項任命一直都      1.0      1.0  4.435080\n",
      "34783  願與大陸維繫      1.0      1.0  4.435080\n",
      "34784  顯示她並沒有      1.0      1.0  4.435080\n",
      "34785  黨當局任命綠      1.0      1.0  4.435080\n",
      "\n",
      "[34786 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# tf- idf \n",
    "# with(1+logtf) * log(doc/df)\n",
    "# use log10\n",
    "\n",
    "for c in case_list[4:5]: # for Taiwan\n",
    "    for k in K:\n",
    "        doc = n[c.topic]\n",
    "        c._words[k].loc[:,'tf-idf'] = (np.log10(c._words[k]['case_tf']) + 1) * np.log10(doc/c._words[k]['case_df'])\n",
    "        print(c.topic, 'k =', k)\n",
    "        print(c._words[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_list[4]._words[2].to_excel('TW.xlsx', sheet_name='2-gram')\n",
    "case_list[4]._words[3].to_excel('TW1.xlsx', sheet_name='3-gram')\n",
    "case_list[4]._words[4].to_excel('TW2.xlsx', sheet_name='4-gram')\n",
    "case_list[4]._words[5].to_excel('TW3.xlsx', sheet_name='5-gram')\n",
    "case_list[4]._words[6].to_excel('TW4.xlsx', sheet_name='6-gram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
