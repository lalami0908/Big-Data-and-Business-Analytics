{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import jieba\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_day(indexs, gap, percentage, listed_stock_2018):\n",
    "    MediaTek = pd.DataFrame(columns = ['date','class'])\n",
    "    print(\"all_days \", len(indexs))\n",
    "    \n",
    "    for i in range(len(indexs) - gap):\n",
    "        j = i + gap\n",
    "        if float(listed_stock_2018.iloc[indexs[i]]['收盤價(元)'])* (1 + percentage) < float(listed_stock_2018.iloc[indexs[j]]['收盤價(元)']):\n",
    "            MediaTek = MediaTek.append([{'date':listed_stock_2018.iloc[indexs[i]]['年月日'],'class': 1}])\n",
    "        elif float(listed_stock_2018.iloc[indexs[i]]['收盤價(元)'])* (1-percentage) > float(listed_stock_2018.iloc[indexs[j]]['收盤價(元)']):\n",
    "            MediaTek = MediaTek.append([{'date':listed_stock_2018.iloc[indexs[i]]['年月日'],'class': -1}])\n",
    "        else:\n",
    "            MediaTek = MediaTek.append([{'date':listed_stock_2018.iloc[indexs[i]]['年月日'],'class': 0}])\n",
    "    return MediaTek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_article(news, deal,MediaTek):\n",
    "    label_news_2018 = pd.DataFrame(columns = ['id','post_time','content','class'])\n",
    "\n",
    "    for index, val in news.iterrows():\n",
    "        if index in deal:\n",
    "            for j in range(len(MediaTek[\"date\"])):\n",
    "                if val[\"post_time\"] == MediaTek[\"date\"].values[j]:\n",
    "                    if MediaTek[\"class\"].values[j] == 1:\n",
    "                        label_news_2018 = label_news_2018.append([{'id':val['id'], 'post_time':val['post_time'],'content':val['content'], 'class':1}])\n",
    "                    elif MediaTek[\"class\"].values[j] == -1:\n",
    "                        label_news_2018 = label_news_2018.append([{'id':val['id'], 'post_time':val['post_time'],'content':val['content'], 'class':0}])\n",
    "                    break\n",
    "\n",
    "    return label_news_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(ver):\n",
    "    print(ver['class'].value_counts())\n",
    "    \n",
    "    ver_up = ver[ver['class'] == 1]\n",
    "    ver_down = ver[ver['class'] == 0]\n",
    "    \n",
    "    text_all = []\n",
    "    label_all = []\n",
    "    text_up = ''\n",
    "    sent_up = []\n",
    "\n",
    "    for i in ver_up['content']:\n",
    "        text_up = i\n",
    "        text_up = re.sub(r'[^\\w]','',text_up)\n",
    "        text_up = re.sub(r'[A-Za-z0-9]','',text_up)\n",
    "        temp_up = ''\n",
    "        segments_up=[]\n",
    "        remainderWords_up=[]\n",
    "        segments_up = jieba.cut(text_up, cut_all=False)\n",
    "        remainderWords_up = list(filter(lambda a: a not in stopword and a != '\\n', segments_up))\n",
    "        cnt = 0\n",
    "        for k in remainderWords_up:\n",
    "            if (cnt == 0) or (cnt == len(remainderWords_up)):\n",
    "                temp_up += (k)\n",
    "            else:\n",
    "                temp_up += ' '\n",
    "                temp_up += k\n",
    "            cnt += 1\n",
    "        sent_up.append(temp_up)\n",
    "    print(\"up: \", len(sent_up))\n",
    "\n",
    "\n",
    "    text_all = []\n",
    "    label_all = []\n",
    "    text_down = ''\n",
    "    sent_down = []\n",
    "    \n",
    "    for i in ver_down['content']:\n",
    "        text_down = i\n",
    "        text_down = re.sub(r'[^\\w]','',text_down)\n",
    "        text_down = re.sub(r'[A-Za-z0-9]','',text_down)\n",
    "        temp_down = ''\n",
    "        segments_down=[]\n",
    "        remainderWords_down=[]\n",
    "        segments_down = jieba.cut(text_down, cut_all=False)\n",
    "        remainderWords_down = list(filter(lambda a: a not in stopword and a != '\\n', segments_down))\n",
    "        cnt = 0\n",
    "        for k in remainderWords_down:\n",
    "            if (cnt == 0) or (cnt == len(remainderWords_down)):\n",
    "                temp_down += (k)\n",
    "            else:\n",
    "                temp_down += ' '\n",
    "                temp_down += k\n",
    "            cnt += 1\n",
    "        sent_down.append(temp_down)\n",
    "    print(\"down: \",len(sent_down))\n",
    "\n",
    "    text_all = []\n",
    "    label_all = []\n",
    "    text_down = ''\n",
    "    sent = []\n",
    "\n",
    "    for i in ver['content']:\n",
    "        text_up = i\n",
    "        text_up = re.sub(r'[^\\w]','',text_up)\n",
    "        text_up = re.sub(r'[A-Za-z0-9]','',text_up)\n",
    "        temp_up = ''\n",
    "        segments_up=[]\n",
    "        remainderWords_up=[]\n",
    "        segments_up = jieba.cut(text_up, cut_all=False)\n",
    "        remainderWords_up = list(filter(lambda a: a not in stopword and a != '\\n', segments_up))\n",
    "        cnt = 0\n",
    "        for k in remainderWords_up:\n",
    "            if (cnt == 0) or (cnt == len(remainderWords_up)):\n",
    "                temp_up += (k)\n",
    "            else:\n",
    "                temp_up += ' '\n",
    "                temp_up += k\n",
    "            cnt += 1\n",
    "        sent.append(temp_up)\n",
    "    print(\"all: \",len(sent))\n",
    "    \n",
    "    return sent_up, sent_down, sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(a, b, c, sent_up ,sent_down):\n",
    "    cv_up = CountVectorizer(min_df = a ,max_df = b ,max_features =c)\n",
    "    cv_fit_up =cv_up.fit_transform(sent_up)\n",
    "    \n",
    "    print(\"up_keyword: \",cv_fit_up.toarray().shape)\n",
    "    key_up = cv_up.get_feature_names()\n",
    "    \n",
    "    cv_down = CountVectorizer(min_df = a ,max_df = b ,max_features =c)\n",
    "    cv_fit_down =cv_down.fit_transform(sent_down)\n",
    "    \n",
    "    print(\"down_keyword: \",cv_fit_down.toarray().shape)\n",
    "    \n",
    "    key_down = cv_down.get_feature_names()\n",
    "    \n",
    "    key = key_up\n",
    "    for i in key_down:\n",
    "        if i not in key_up:\n",
    "            key.append(i)\n",
    "    return key_up, key_down, key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = []\n",
    "with open ('stopword.txt','r',encoding = 'utf-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopword.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"news.csv\",encoding = \"big5\")\n",
    "n = len(news)\n",
    "news.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news)):\n",
    "    news.iloc[i]['post_time'] = news.iloc[i]['post_time'].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_stock_2018 = pd.read_csv(\"listed_stock_2018.csv\")\n",
    "listed_stock_2017 = pd.read_csv(\"listed_stock_2017.csv\")\n",
    "listed_stock_2016 = pd.read_csv(\"listed_stock_2016.csv\")\n",
    "listed_stock_2018 = listed_stock_2018.append(listed_stock_2017,ignore_index = True)\n",
    "listed_stock_2018 = listed_stock_2018.append(listed_stock_2016,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all days:  737\n"
     ]
    }
   ],
   "source": [
    "find = listed_stock_2018['證券代碼'] == \"2454 聯發科\"\n",
    "indexs = list(find[find.values == True].index)\n",
    "indexs.reverse()\n",
    "print(\"all days: \", len(indexs))\n",
    "\n",
    "# for i in indexs:\n",
    "#     print(listed_stock_2018.iloc[i]['年月日'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = \"聯發科\"\n",
    "deal = [] # remember the articles index will be dealed with\n",
    "#article_list = [article(index) for index in range(n+1)] # extra new for index = 0 \n",
    "\n",
    "for index, val in news.iterrows(): # for 90000\n",
    "    if stock in val[\"content\"] + val[\"title\"]:\n",
    "        deal.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all articles:  5185\n",
      "[69, 147, 238, 290, 307, 371, 441, 485, 486, 517]\n"
     ]
    }
   ],
   "source": [
    "print(\"all articles: \", len(deal))\n",
    "print(deal[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_days  737\n",
      " 0    655\n",
      "-1     41\n",
      " 1     40\n",
      "Name: class, dtype: int64\n",
      "736\n",
      "0    283\n",
      "1    262\n",
      "Name: class, dtype: int64\n",
      "up:  262\n",
      "down:  283\n",
      "all:  545\n",
      "up_keyword:  (262, 2000)\n",
      "down_keyword:  (283, 2000)\n"
     ]
    }
   ],
   "source": [
    "MediaTek = label_day(indexs, 1, 0.03,  listed_stock_2018)\n",
    "label_news_2018 = label_article(news, deal, MediaTek)  \n",
    "all_article = token(label_news_2018) # sent_up,sent_down,sent\n",
    "key = find_keyword(0.01, 0.6, 2000, all_article[0], all_article[1]) # key_up, key_down, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(545, 2602)\n",
      "valid : 436\n"
     ]
    }
   ],
   "source": [
    "# all\n",
    "cv = CountVectorizer(vocabulary = key[2])\n",
    "cv_fit =cv.fit_transform(all_article[2])\n",
    "all = len(cv_fit.toarray())\n",
    "print(cv_fit.toarray().shape)\n",
    "\n",
    "X_data = cv_fit.toarray()\n",
    "Y_data = label_news_2018['class'].to_numpy(dtype=int)\n",
    "\n",
    "valid = int(math.floor(all * 0.8)) # Round down\n",
    "print('valid :', valid)\n",
    "\n",
    "X_train = X_data[:valid]\n",
    "X_test = X_data[valid:]\n",
    "\n",
    "Y_train = Y_data[:valid]\n",
    "Y_test = Y_data[valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[39 27]\n",
      " [32 11]]\n",
      "R= 0.2558139534883721\n",
      "P= 0.2894736842105263\n",
      "Accuracy= 0.45871559633027525\n",
      "F1= 0.2716049382716049\n",
      "2\n",
      "[[57  9]\n",
      " [38  5]]\n",
      "R= 0.11627906976744186\n",
      "P= 0.35714285714285715\n",
      "Accuracy= 0.5688073394495413\n",
      "F1= 0.1754385964912281\n",
      "3\n",
      "[[52 14]\n",
      " [33 10]]\n",
      "R= 0.23255813953488372\n",
      "P= 0.4166666666666667\n",
      "Accuracy= 0.5688073394495413\n",
      "F1= 0.29850746268656714\n",
      "4\n",
      "[[60  6]\n",
      " [38  5]]\n",
      "R= 0.11627906976744186\n",
      "P= 0.45454545454545453\n",
      "Accuracy= 0.5963302752293578\n",
      "F1= 0.18518518518518515\n",
      "5\n",
      "[[56 10]\n",
      " [36  7]]\n",
      "R= 0.16279069767441862\n",
      "P= 0.4117647058823529\n",
      "Accuracy= 0.5779816513761468\n",
      "F1= 0.23333333333333336\n",
      "10\n",
      "[[62  4]\n",
      " [38  5]]\n",
      "R= 0.11627906976744186\n",
      "P= 0.5555555555555556\n",
      "Accuracy= 0.6146788990825688\n",
      "F1= 0.19230769230769232\n",
      "15\n",
      "[[44 22]\n",
      " [30 13]]\n",
      "R= 0.3023255813953488\n",
      "P= 0.37142857142857144\n",
      "Accuracy= 0.5229357798165137\n",
      "F1= 0.3333333333333333\n",
      "20\n",
      "[[35 31]\n",
      " [25 18]]\n",
      "R= 0.4186046511627907\n",
      "P= 0.3673469387755102\n",
      "Accuracy= 0.48623853211009177\n",
      "F1= 0.391304347826087\n",
      "25\n",
      "[[25 41]\n",
      " [19 24]]\n",
      "R= 0.5581395348837209\n",
      "P= 0.36923076923076925\n",
      "Accuracy= 0.44954128440366975\n",
      "F1= 0.4444444444444445\n",
      "30\n",
      "[[35 31]\n",
      " [25 18]]\n",
      "R= 0.4186046511627907\n",
      "P= 0.3673469387755102\n",
      "Accuracy= 0.48623853211009177\n",
      "F1= 0.391304347826087\n",
      "35\n",
      "[[41 25]\n",
      " [26 17]]\n",
      "R= 0.3953488372093023\n",
      "P= 0.40476190476190477\n",
      "Accuracy= 0.5321100917431193\n",
      "F1= 0.4\n",
      "40\n",
      "[[48 18]\n",
      " [29 14]]\n",
      "R= 0.32558139534883723\n",
      "P= 0.4375\n",
      "Accuracy= 0.5688073394495413\n",
      "F1= 0.37333333333333335\n",
      "45\n",
      "[[53 13]\n",
      " [25 18]]\n",
      "R= 0.4186046511627907\n",
      "P= 0.5806451612903226\n",
      "Accuracy= 0.6513761467889908\n",
      "F1= 0.4864864864864865\n",
      "50\n",
      "[[52 14]\n",
      " [30 13]]\n",
      "R= 0.3023255813953488\n",
      "P= 0.48148148148148145\n",
      "Accuracy= 0.5963302752293578\n",
      "F1= 0.37142857142857144\n"
     ]
    }
   ],
   "source": [
    "K = [1,2,3,4,5,10,15,20,25,30,35,40,45,50]\n",
    "for k in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,Y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    print(k)\n",
    "    print(confusion_matrix(Y_test,pred))\n",
    "    \n",
    "    print('R=',metrics.recall_score(Y_test, pred))\n",
    "    print('P=',metrics.precision_score(Y_test, pred))\n",
    "    print('Accuracy=',metrics.accuracy_score(Y_test, pred))\n",
    "    print('F1=',metrics.f1_score(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
